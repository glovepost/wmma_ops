{
  "url": "https://api.github.com/repos/ROCm/composable_kernel/issues/1434",
  "repository_url": "https://api.github.com/repos/ROCm/composable_kernel",
  "labels_url": "https://api.github.com/repos/ROCm/composable_kernel/issues/1434/labels{/name}",
  "comments_url": "https://api.github.com/repos/ROCm/composable_kernel/issues/1434/comments",
  "events_url": "https://api.github.com/repos/ROCm/composable_kernel/issues/1434/events",
  "html_url": "https://github.com/ROCm/composable_kernel/issues/1434",
  "id": 2443596995,
  "node_id": "I_kwDOFz9uWc6RplTD",
  "number": 1434,
  "title": "WMMA / RDNA3+ kernels for backwards fused attention?",
  "user": {
    "login": "Googulator",
    "id": 16308406,
    "node_id": "MDQ6VXNlcjE2MzA4NDA2",
    "avatar_url": "https://avatars.githubusercontent.com/u/16308406?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/Googulator",
    "html_url": "https://github.com/Googulator",
    "followers_url": "https://api.github.com/users/Googulator/followers",
    "following_url": "https://api.github.com/users/Googulator/following{/other_user}",
    "gists_url": "https://api.github.com/users/Googulator/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/Googulator/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Googulator/subscriptions",
    "organizations_url": "https://api.github.com/users/Googulator/orgs",
    "repos_url": "https://api.github.com/users/Googulator/repos",
    "events_url": "https://api.github.com/users/Googulator/events{/privacy}",
    "received_events_url": "https://api.github.com/users/Googulator/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "labels": [
    {
      "id": 7354310647,
      "node_id": "LA_kwDOFz9uWc8AAAABtlnf9w",
      "url": "https://api.github.com/repos/ROCm/composable_kernel/labels/Under%20Investigation",
      "name": "Under Investigation",
      "color": "2F1BFE",
      "default": false,
      "description": ""
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 16,
  "created_at": "2024-08-01T22:32:41Z",
  "updated_at": "2025-07-18T12:49:43Z",
  "closed_at": null,
  "author_association": "NONE",
  "type": null,
  "active_lock_reason": null,
  "sub_issues_summary": {
    "total": 0,
    "completed": 0,
    "percent_completed": 0
  },
  "issue_dependencies_summary": {
    "blocked_by": 0,
    "total_blocked_by": 0,
    "blocking": 0,
    "total_blocking": 0
  },
  "body": "### Problem Description\n\nComposable Kernel currently only contains code to support fused attention (FA2) on RDNA3(+) architectures in the forward direction. This greatly increases the VRAM requirements for training LoRAs on LLMs using HuggingFace's Transformers and PEFT libraries - training jobs that succeed on an NVIDIA GeForce RTX 4080 with just 16GB VRAM fail on a Radeon RX 7900 XT with 20GB.\r\n\r\nBased on https://github.com/Repeerc/flash-attention-v2-RDNA3-minimal and https://github.com/Repeerc/sd-webui-flash-attention2-rdna3-rocm, it seems possible to implement a usable WMMA-based backwards fused attention kernel - unfortunately I can't use these myself directly, as these are both tailored for image generation (Stable Diffusion), whereas I would be interested in FA2 support for LLM training instead.\r\n\r\nAre there any plans for adding fused attention backward pass support for RDNA3+ GPUs to CK in the foreseeable future? This seems especially pressing with the W7900 Dual Slot, an RDNA3 GPU, being recommended for AI workstation usage, where the ability to make effective use of this GPU's 48GB VRAM during training feels a lot more of a core use case.\n\n### Operating System\n\nUbuntu 22.04 LTS\n\n### CPU\n\nAMD Ryzen 9 7950X (non-3D)\n\n### GPU\n\nAMD Radeon RX 7900 XTX, AMD Radeon Pro W7900, AMD Radeon Pro W7800, AMD Radeon RX 7900 XT\n\n### Other\n\n_No response_\n\n### ROCm Version\n\nROCm 6.0.0\n\n### ROCm Component\n\nComposable Kernel\n\n### Steps to Reproduce\n\n_No response_\n\n### (Optional for Linux users) Output of /opt/rocm/bin/rocminfo --support\n\n_No response_\n\n### Additional Information\n\n_No response_",
  "closed_by": {
    "login": "schung-amd",
    "id": 175627365,
    "node_id": "U_kgDOCnfcZQ",
    "avatar_url": "https://avatars.githubusercontent.com/u/175627365?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/schung-amd",
    "html_url": "https://github.com/schung-amd",
    "followers_url": "https://api.github.com/users/schung-amd/followers",
    "following_url": "https://api.github.com/users/schung-amd/following{/other_user}",
    "gists_url": "https://api.github.com/users/schung-amd/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/schung-amd/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/schung-amd/subscriptions",
    "organizations_url": "https://api.github.com/users/schung-amd/orgs",
    "repos_url": "https://api.github.com/users/schung-amd/repos",
    "events_url": "https://api.github.com/users/schung-amd/events{/privacy}",
    "received_events_url": "https://api.github.com/users/schung-amd/received_events",
    "type": "User",
    "user_view_type": "public",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/ROCm/composable_kernel/issues/1434/reactions",
    "total_count": 2,
    "+1": 2,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ROCm/composable_kernel/issues/1434/timeline",
  "performed_via_github_app": null,
  "state_reason": "reopened"
}
