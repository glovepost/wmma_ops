# ROCm 7.9 Preview Benchmarking Container for gfx1151 (Strix Halo / Ryzen AI Max)
# Purpose: Dedicated HuggingFace model benchmarking and testing
#
# Installation: Uses pip-based ROCm 7.9 for Ryzen APU (gfx1151)
# Reference: https://rocm.docs.amd.com/en/7.9.0-preview/install/rocm.html

FROM ubuntu:24.04

# Prevent interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# GPU/ROCm environment
# Note: HSA_OVERRIDE_GFX_VERSION not needed for ROCm 7.10+
ENV PYTORCH_ROCM_ARCH=gfx1151
ENV HIP_VISIBLE_DEVICES=0

# ===========================================
# PERFORMANCE OPTIMIZATIONS (set at build time)
# ===========================================

# TunableOp - CRITICAL: +57% GEMM improvement
ENV PYTORCH_TUNABLEOP_ENABLED=1
ENV PYTORCH_TUNABLEOP_TUNING=1
ENV PYTORCH_TUNABLEOP_VERBOSE=0

# hipBLASLt - Prefer optimized BLAS library
ENV TORCH_BLAS_PREFER_HIPBLASLT=1

# Flash Attention via AOTriton
ENV TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1

# Memory allocation (updated - removed unsupported expandable_segments)
ENV PYTORCH_ALLOC_CONF=garbage_collection_threshold:0.9,max_split_size_mb:512

# AMD RDNA stability
ENV HIP_FORCE_DEV_KERNARG=1

# Python environment
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Install base dependencies (minimal for Docker as per docs)
RUN apt-get update && apt-get install -y --no-install-recommends \
    sudo \
    wget \
    curl \
    git \
    ca-certificates \
    python3.12 \
    python3.12-venv \
    python3.12-dev \
    build-essential \
    cmake \
    gnupg2 \
    && rm -rf /var/lib/apt/lists/*

# Add ROCm APT repository and install rocm-llvm for hipcc compilation
RUN mkdir -p /etc/apt/keyrings && \
    wget -q -O - https://repo.radeon.com/rocm/rocm.gpg.key | gpg --dearmor > /etc/apt/keyrings/rocm.gpg && \
    echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/rocm/apt/6.3 jammy main" > /etc/apt/sources.list.d/rocm.list && \
    printf 'Package: *\nPin: release o=repo.radeon.com\nPin-Priority: 600\n' > /etc/apt/preferences.d/rocm-pin-600

# Install rocm-llvm (includes clang needed by hipcc), hipcc, rocprofiler, and thrust headers
RUN apt-get update && apt-get install -y --no-install-recommends \
    rocm-llvm \
    hipcc \
    rocm-hip-runtime-dev \
    rocthrust-dev \
    rocprofiler-dev \
    roctracer-dev \
    && rm -rf /var/lib/apt/lists/*

# Add HIP to PATH and libraries to LD_LIBRARY_PATH
# Prioritize pip-installed SDK libs (ROCm 7.x) over system/APT libs (ROCm 6.3) to prevent ABI mismatch
ENV PATH="/opt/rocm/bin:/opt/rocm/llvm/bin:$PATH"
ENV HIP_PATH="/opt/rocm"
ENV ROCM_PATH="/opt/rocm"
ENV LD_LIBRARY_PATH="/opt/venv/lib/python3.12/site-packages/_rocm_sdk_core/lib:/opt/venv/lib/python3.12/site-packages/_rocm_sdk_core/lib/host-math/lib:/opt/venv/lib/python3.12/site-packages/_rocm_sdk_core/lib/roctracer/lib:/opt/venv/lib/python3.12/site-packages/_rocm_sdk_core/lib/rocm_sysdeps/lib:/opt/venv/lib/python3.12/site-packages/_rocm_sdk_core/lib/llvm/lib:/opt/venv/lib/python3.12/site-packages/_rocm_sdk_libraries_gfx1151/lib:/opt/rocm/lib:/opt/rocm/hip/lib:$LD_LIBRARY_PATH"

# Make python3.12 the default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1

# Create working directory
WORKDIR /workspace

# Create Python virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip
RUN pip install --upgrade pip setuptools wheel

# Install ROCm 7.9 via pip (gfx1151 wheel for Ryzen AI Max)
RUN pip install --index-url https://repo.amd.com/rocm/whl/gfx1151/ "rocm[libraries,devel]"

# Install PyTorch from gfx1151-optimized wheel
RUN pip install --index-url https://repo.amd.com/rocm/whl/gfx1151/ \
    torch \
    torchvision \
    torchaudio

# Install HuggingFace and benchmarking dependencies
RUN pip install \
    transformers \
    accelerate \
    datasets \
    safetensors \
    sentencepiece \
    tokenizers \
    huggingface-hub \
    scipy \
    numpy \
    pandas \
    tqdm \
    jsonlines \
    rich \
    py-cpuinfo \
    psutil \
    peft \
    triton \
    einops

# Install flash-attention for models that require it (Phi-3, etc.)
# Try ROCm-compatible build, fall back if unavailable
RUN pip install flash-attn --no-build-isolation || \
    echo "flash-attn install failed - models will use eager attention"

# Install mamba-ssm from source with triton backend for ROCm
RUN pip install causal-conv1d --no-build-isolation || true
RUN MAMBA_FORCE_BUILD=TRUE MAMBA_SKIP_CUDA_BUILD=TRUE pip install mamba-ssm --no-build-isolation || \
    echo "mamba-ssm install failed - Nemotron-3 will not be available"

# Create benchmarking scripts directory
RUN mkdir -p /workspace/benchmarks

# Create TunableOp cache directory (crucial for persistence)
RUN mkdir -p /tmp/tunableop

# Environment check script
COPY --chmod=755 <<'EOF' /workspace/benchmarks/check_env.py
#!/usr/bin/env python3
"""Verify PyTorch ROCm environment."""
import subprocess
import sys

print("=" * 60)
print("ROCm 7.9 Benchmarking Environment Check")
print("=" * 60)

# Check rocm-sdk
print("\n[1] ROCm SDK:")
try:
    result = subprocess.run(["rocm-sdk", "targets"], capture_output=True, text=True)
    print(result.stdout if result.returncode == 0 else "rocm-sdk not available")
except FileNotFoundError:
    print("rocm-sdk not found in PATH")

# Check rocminfo
print("\n[2] rocminfo:")
try:
    result = subprocess.run(["rocminfo"], capture_output=True, text=True)
    for line in result.stdout.split('\n'):
        if 'Name:' in line or 'gfx' in line:
            print(f"  {line.strip()}")
except FileNotFoundError:
    print("rocminfo not found")

# Check PyTorch
print("\n[3] PyTorch:")
import torch
print(f"  Version: {torch.__version__}")
print(f"  ROCm available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"  Device count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  Device {i}: {torch.cuda.get_device_name(i)}")
        props = torch.cuda.get_device_properties(i)
        print(f"    Memory: {props.total_memory / 1024**3:.1f} GB")

# Quick benchmark
print("\n[4] Quick MatMul Benchmark:")
if torch.cuda.is_available():
    import time
    x = torch.randn(4096, 4096, device='cuda', dtype=torch.float16)
    # Warmup
    for _ in range(5):
        _ = torch.mm(x, x)
    torch.cuda.synchronize()
    # Benchmark
    start = time.time()
    for _ in range(100):
        _ = torch.mm(x, x)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    tflops = (2 * 4096**3 * 100) / elapsed / 1e12
    print(f"  FP16 4096x4096 MatMul: {tflops:.2f} TFLOPS")

print("\n" + "=" * 60)
print("Environment check complete!")
EOF

# Model benchmark script
COPY --chmod=755 <<'EOF' /workspace/benchmarks/bench_model.py
#!/usr/bin/env python3
"""Benchmark HuggingFace model loading and inference."""
import argparse
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"])
    parser.add_argument("--max_new_tokens", type=int, default=100)
    parser.add_argument("--prompt", type=str, default="The capital of France is")
    args = parser.parse_args()

    dtype_map = {"float16": torch.float16, "bfloat16": torch.bfloat16, "float32": torch.float32}
    dtype = dtype_map[args.dtype]

    print(f"Loading model: {args.model}")
    load_start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.model, torch_dtype=dtype, device_map="auto", trust_remote_code=True
    )

    print(f"Model loaded in {time.time() - load_start:.2f}s")

    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"GPU Memory: {allocated:.2f} GB allocated")

    # Inference benchmark
    print(f"\nBenchmarking {args.max_new_tokens} tokens...")
    inputs = tokenizer(args.prompt, return_tensors="pt").to(model.device)

    # Warmup
    with torch.no_grad():
        _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)

    # Benchmark
    times = []
    for _ in range(5):
        torch.cuda.synchronize()
        start = time.time()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=args.max_new_tokens, do_sample=False)
        torch.cuda.synchronize()
        times.append(time.time() - start)

    avg_time = sum(times) / len(times)
    tokens_per_sec = args.max_new_tokens / avg_time

    print(f"\nResults:")
    print(f"  Time: {avg_time:.3f}s")
    print(f"  Tokens/sec: {tokens_per_sec:.1f}")
    print(f"  Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)[:200]}...")

if __name__ == "__main__":
    main()
EOF

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import torch; assert torch.cuda.is_available()"

CMD ["python", "/workspace/benchmarks/check_env.py"]
